{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_2_text_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld9Ptqk-zDnU"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.optimizers import RMSprop, adam\n",
        "from keras import preprocessing\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.datasets import imdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLC0aEaED9hx"
      },
      "source": [
        "Dataset this time is the imdb review set, how do we evaluate the sentiment expressed by people who bother to write movie reviews?  At some level this is a harder problem than we may expect - if we're not careful with how we build our model we may end up ranking a review like \"this movie is the bomb\" the same as \"this movie is a bomb\" even though they clearly express different sentiments.  Our first model will purposefully suffer from this problem as an illustration of what to avoid.\n",
        "\n",
        "There are a few ways to skin this cat, and you've seen one of the less efficient ones with the statistical NLP approach already.  The other two ones at this stage are one hot encoding (creating large numbers of sparse vectors to represent words in a text) and word embeddings (dense, low-dimensional and data specific.)  For a scale comparison, you'll expect to see word embeddings with a few hundreds to low thousands of dimensions while one hot encoding produces dimensionality on the order of tens of thousands(!!!) We'll be working with the latter.  \n",
        "\n",
        "Broadly speaking there are two ways to learn and use word embeddings: learn them on the fly or use pretrained embeddings.  We'll examine both, but in practice will generally rely on the latter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn-OD0T36Pi7",
        "outputId": "8249160b-573e-4cb5-eefd-8e035c0aafc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#These are some variables we're setting for easy maneuver later, don't worry about their names now\n",
        "max_features = 10000\n",
        "maxlen = 20\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
        "\n",
        "#A drawback of a static setup like what we're about to do is that the inputs are expected\n",
        "#to be of a consistent size - in this case 20 words.  To accomodate shorter sentences\n",
        "#we simply pad the length of all sentences to match this length limit.\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen = maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen = maxlen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWkSLSrT6Sne",
        "outputId": "e9734eda-56f2-4a02-e59c-832729fbc6bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "#Build our model\n",
        "model = Sequential()\n",
        "#This is the first time we've used the Embedding layer.  For simplicity, we can\n",
        "#think of it as a dictionary that maps integer indices to dense vectors.  As\n",
        "#input we take a 2D tensor (samples, sequence_length) and returns a 3D tensor\n",
        "#(samples, sequence_length, embedding_dim)\n",
        "model.add(Embedding(10000, 8, input_length = maxlen)) #(10000 samples, maxlen, 8) \n",
        "model.add(Flatten()) #2D tensor (samples, maxlen * 8)\n",
        "model.add(Dense(1, activation = 'sigmoid')) #1D probability output\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 161       \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYW6XX8U-5mg",
        "outputId": "fb6e7bdf-fee0-40a4-ad0f-129ae8528b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "model.fit(x_train, y_train, epochs = 10, batch_size = 32, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 2s 76us/step - loss: 0.6552 - accuracy: 0.6420 - val_loss: 0.5768 - val_accuracy: 0.7166\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 1s 61us/step - loss: 0.4813 - accuracy: 0.7866 - val_loss: 0.4967 - val_accuracy: 0.7512\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 1s 61us/step - loss: 0.3847 - accuracy: 0.8375 - val_loss: 0.4888 - val_accuracy: 0.7542\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 1s 60us/step - loss: 0.3215 - accuracy: 0.8742 - val_loss: 0.4979 - val_accuracy: 0.7532\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 1s 61us/step - loss: 0.2682 - accuracy: 0.9027 - val_loss: 0.5180 - val_accuracy: 0.7464\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 1s 60us/step - loss: 0.2215 - accuracy: 0.9266 - val_loss: 0.5389 - val_accuracy: 0.7442\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 1s 61us/step - loss: 0.1806 - accuracy: 0.9462 - val_loss: 0.5688 - val_accuracy: 0.7382\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 1s 62us/step - loss: 0.1457 - accuracy: 0.9617 - val_loss: 0.5995 - val_accuracy: 0.7364\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 1s 62us/step - loss: 0.1168 - accuracy: 0.9730 - val_loss: 0.6333 - val_accuracy: 0.7324\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 1s 61us/step - loss: 0.0923 - accuracy: 0.9818 - val_loss: 0.6716 - val_accuracy: 0.7306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1e1baff400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM7yyizq_fOL",
        "outputId": "404f3328-5288-44a3-f34c-383ce72275dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 0s 18us/step\n",
            "0.735759973526001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRwC-2ayfOQe"
      },
      "source": [
        "So without any real structure to our network and a gimped view of the data, e.g. considerations of relationships between words, context, looking at only the first 20 words in a review, etc., we scored around 73% accuracy in our automated review checker.  We can improve this by using 1D convnets or Recurrent layers in our next iteration.  Let's start with 1D convnets.\n",
        "\n",
        "We looked at convnets last time to deal with picture data, and found it valuable for extracting local features and building modular and efficient data representations.  We can make use of these features by considering our text sequence to be something like a stream of time flowing forwards.  We'll find later that RNN's are generally better at text tasks, but small 1D convnets can perform better and more quickly for simple tasks.\n",
        "\n",
        "Whereas previously we were considering patches of an image with our convnets, now we are looking at subsequences of our sequences.  These layers will help to recognize local patterns of a larger sentence.  For example, in the ideal, a window of size three should be able to learn words or word fragments of length three or less, and should be able to recognize that pattern anywhere else in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AugxAYwy_9qG"
      },
      "source": [
        "#We'll implement a new model for comparison down here.\n",
        "max_features = 10000\n",
        "max_len = 50\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen = max_len)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrXqhHC3lqHS",
        "outputId": "73012900-179b-41ca-dba6-5b49161c77e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "cnn = Sequential()\n",
        "cnn.add(Embedding(max_features, 128, input_length = max_len))\n",
        "cnn.add(Conv1D(32, 7, activation = 'relu')) #Note the different window size here, we can afford a large one now at 7\n",
        "cnn.add(MaxPooling1D(5))\n",
        "cnn.add(Conv1D(32, 7, activation = 'relu'))\n",
        "cnn.add(GlobalMaxPooling1D())\n",
        "cnn.add(Dense(1))\n",
        "\n",
        "cnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 50, 128)           1280000   \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 44, 32)            28704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 8, 32)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 2, 32)             7200      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,315,937\n",
            "Trainable params: 1,315,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIjMkZZFnTsT",
        "outputId": "3b9f7746-6f1c-4fb6-c615-02754ee33552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "cnn.compile(optimizer = adam(lr=1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "cnn.fit(x_train, y_train, epochs = 8, batch_size = 128, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/8\n",
            "20000/20000 [==============================] - 9s 460us/step - loss: 0.8231 - accuracy: 0.5099 - val_loss: 0.6850 - val_accuracy: 0.5596\n",
            "Epoch 2/8\n",
            "20000/20000 [==============================] - 9s 455us/step - loss: 0.6717 - accuracy: 0.6216 - val_loss: 0.6752 - val_accuracy: 0.6022\n",
            "Epoch 3/8\n",
            "20000/20000 [==============================] - 9s 454us/step - loss: 0.6472 - accuracy: 0.6991 - val_loss: 0.6602 - val_accuracy: 0.6416\n",
            "Epoch 4/8\n",
            "20000/20000 [==============================] - 9s 451us/step - loss: 0.6133 - accuracy: 0.7572 - val_loss: 0.6334 - val_accuracy: 0.6838\n",
            "Epoch 5/8\n",
            "20000/20000 [==============================] - 9s 454us/step - loss: 0.5583 - accuracy: 0.7932 - val_loss: 0.5841 - val_accuracy: 0.7124\n",
            "Epoch 6/8\n",
            "20000/20000 [==============================] - 9s 456us/step - loss: 0.4684 - accuracy: 0.8122 - val_loss: 0.5427 - val_accuracy: 0.7470\n",
            "Epoch 7/8\n",
            "20000/20000 [==============================] - 9s 454us/step - loss: 0.3781 - accuracy: 0.8462 - val_loss: 0.5795 - val_accuracy: 0.7610\n",
            "Epoch 8/8\n",
            "20000/20000 [==============================] - 9s 451us/step - loss: 0.3121 - accuracy: 0.8727 - val_loss: 0.6220 - val_accuracy: 0.7654\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1dc0cffdd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phQMRopfsWV9",
        "outputId": "d2fb22d7-3456-4c7b-848d-15f3fab8abc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "test_loss, test_acc = cnn.evaluate(x_test, y_test)\n",
        "\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 2s 94us/step\n",
            "0.7619600296020508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylh3aAuQX0LH"
      },
      "source": [
        "So we got about a 3% absolute improvement for what seems like a fairly significant increase in complexity.  WHat are the lessons here?\n",
        "\n",
        "1) We can move away from default settings, and often should.  I encourage you to try to run this notebook with a default learning rate.  This is also a hard problem, don't be discouraged by the low accuracy we're getting.\n",
        "\n",
        "2) CNN's may give us a local sense of information, but we lose this beyond the window.  RNN will alleviate this to a degree.\n",
        "\n",
        "3) We did not try regularization, which may have helped us bridge the accuracy gap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtJYX_8zQi6R"
      },
      "source": [
        
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kicvW7gs23WY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
